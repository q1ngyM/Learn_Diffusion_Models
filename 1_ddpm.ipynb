{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating 16x16 images of Sprites\n",
    "\n",
    "This is a learning note of Diffusion models. The content of this Note includes:\n",
    " - Principle of DDPM\n",
    "   - The explaination the principle of the algorithms for training and sampling(inference) stage proposed by [1]. \n",
    " - Implementation of Diffusion Model with the structure of:\n",
    "   - A UNet as noise predictor\n",
    "\n",
    "In this notebook, I am going to train a diffusion model to generate sprite images with the structure as follows:\n",
    "\n",
    "[image]\n",
    "\n",
    "And I am also going to explain the principle of my implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Overview\n",
    "The Denoising Diffusion Probabilistic Model (DDPM) consists of two main processes:\n",
    "\n",
    "1. **Forward Process (Diffusion Process)**  \n",
    "   In this stage, as is shown in the image, you might think that noise is gradually added to the input image.   \n",
    "   However, Instead of iteratively adding noise step by step, we can directly obtain a noised image $ I_t $ from the original image $ I_0 $ using a **predefined noise schedule**.   \n",
    "   *We will explain this in more detail later.*\n",
    "\n",
    "   <img src=\"images\\fwd.png\" width=\"80%\">\n",
    "\n",
    "2. **Reverse Process (Denoising Process)**  \n",
    "   This stage aims to remove noise from a given noisy image to recover a clean image.   \n",
    "   To achieve this, we train a **Noise Predictor** (typically a neural network) that learns to predict the noise added at each step.   \n",
    "   Once the noise is estimated, we subtract it from the noisy image to progressively reconstruct the original clean image.\n",
    "\n",
    "   <img src=\"images\\rvs.png\" width=\"80%\">\n",
    "\n",
    "In summary:\n",
    "- The forward process produces a set of progressively noised images from $ I_0 $ using a predefined sequence of noise scaling factors $ \\{\\alpha_t\\}_{t=1}^{T} $ and Gaussian noise. Hence, this stage is fixed and does not require training.\n",
    "- The reverse process involves training a Noise Predictor to learn the noise patterns effectively therefore estimating and removing the added noise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Why we can directly obtain a noised image $I_t$?\n",
    "Suppose we add noise step by step:  \n",
    "\n",
    "Given a clean image $ I_0 $ and a full Gaussian noise image $ \\epsilon_0 \\sim \\mathcal{N}(0, I) $, the first noised image can be expressed as:  \n",
    "\n",
    "$ I_1 = \\sqrt{1-\\beta_1} I_0 + \\sqrt{  \\beta_1} \\epsilon_0 $\n",
    "\n",
    "The rest of the images will perform identically:\n",
    "\n",
    "$ I_2 = \\sqrt{1- \\beta_2} I_1 + \\sqrt{   \\beta_2} \\epsilon_1 $  \n",
    "...  \n",
    "$ I_t = \\sqrt{1- \\beta_t} I_{t-1} + \\sqrt{   \\beta_t} \\epsilon_{t-1}$\n",
    "\n",
    "Note that $ \\beta_i $ values are **predifined hyperparameters** controlling the amount of noise added to the image at each step. As shown in the figure, if $ I_T $ is the fully noised image, $ \\beta_i (i\\in[1,T]) $ should **increase** progressively.\n",
    "\n",
    "\n",
    "<img src=\"images\\f1.png\" width=\"80%\">\n",
    "\n",
    "To simplify this step-by-step noise accumulation, we analyze how the noise terms propagate. Since $ \\epsilon_i, i \\in [0,t-1] \\sim \\mathcal{N}(0, I) $, we can express the accumulated noise as:\n",
    "\n",
    "$$ \\sqrt{1-\\beta_2}\\sqrt{\\beta_1}\\epsilon_0 + \\sqrt{\\beta_2}\\epsilon_1 = \\sqrt{1-(1-\\beta_1)(1-\\beta_2)} \\epsilon $$\n",
    "\n",
    "Since both noise terms come from independent Gaussian distributions, their sum remains Gaussian with a new variance.\n",
    "The combined variance follows from the property that multiplying independent Gaussian noise terms by deterministic coefficients results in a weighted sum of variances.\n",
    "\n",
    "\n",
    "Thus, the general equation simplifies to:\n",
    "\n",
    "$$ I_t = \\sqrt{(1-\\beta_1)(1-\\beta_2)...(1-\\beta_t)} I_0 + \\sqrt{1-(1-\\beta_1)(1-\\beta_2)...(1-\\beta_t)} \\epsilon  \\tag{1}$$\n",
    "where $ \\epsilon \\sim \\mathcal{N}(0, I) $.\n",
    "\n",
    "To simplify this equation, suppose $\\alpha_i = 1 - \\beta_i, i\\in[0,T];\\bar{\\alpha_t} = \\alpha_1\\alpha_2 ... \\alpha_t$.\n",
    "Hence, equation $ (1) $ can be rewritten as:\n",
    "$$\n",
    "I_t = \\sqrt{\\bar{\\alpha_t}} I_0 + \\sqrt{1-\\bar{\\alpha_t}} \\epsilon \\tag{2}$$\n",
    "where $ \\epsilon \\sim \\mathcal{N}(0, I)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. How to train the Noise Predictor?\n",
    "In the last section, we've learned how to obtain the noised images, which serve as the supervision (ground truth), given the clean image $ I_0 $. We need to train a **Noise Predictor** model, denoted as $ \\epsilon_\\theta $, to approximate the ground truth noise $ \\epsilon $.\n",
    "\n",
    "Check the algorithm proposed by [1]:\n",
    "\n",
    "<img src=\"images\\eq.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Implementation 1: Training a UNet\n",
    "With the given algorithm, we could train an actual Noise Predictor -- UNet!  \n",
    "The structure of a **simplified** UNet with input dimensions of (16,16,3) is depicted as follows:\n",
    "\n",
    "<img src=\"images\\sunet.png\" width=\"60%\">\n",
    "\n",
    "The goal of a UNet is to predict noise by first downsampling the image to embeddings in the latent space, then upsampling it to predict the noise.\n",
    "\n",
    "\n",
    "We can construct our neural network following this architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels: int, out_channels: int, is_res: bool=False\n",
    "    )->None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Check if input and output channels are the same for the residual conection\n",
    "        self.same_channels = in_channels == out_channels\n",
    "\n",
    "        self.is_res = is_res\n",
    "\n",
    "        # First convolutional layer\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),   \n",
    "            nn.BatchNorm2d(out_channels),   \n",
    "            nn.GELU(),   # TODO why GeLU\n",
    "        )\n",
    "        # Second convolutional layer\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU()\n",
    "        )\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x1)\n",
    "\n",
    "        if self.is_res:\n",
    "            if self.same_channels:\n",
    "                out = x + x2\n",
    "            else:\n",
    "                # if the dimension of input and output channels are not the same, \n",
    "                # apply a 1x1 conv layer to match dimension before res connecting\n",
    "                shortcut = nn.Conv2d(x.shape[1], x2.shape[1], kernel_size = 1, stride=1, padding=1).to(x.device)\n",
    "                out = shortcut(x) + x2\n",
    "            print(f\"res conv forward: x {x.shape}, x1 {x1.shape}, x2 {x2.shape}, out {out.shape}\")\n",
    "\n",
    "            # normalization: divide by sqrt(2)\n",
    "            return out / 1.414\n",
    "        \n",
    "        else:\n",
    "            return x2\n",
    "    \n",
    "    def get_out_channels(self):\n",
    "        return self.conv2[0].out_channels\n",
    "    \n",
    "    def set_out_channels(self, out_channels):\n",
    "        self.conv1[0].out_channels = out_channels\n",
    "        self.conv2[0].in_channels = out_channels\n",
    "        self.conv1[0].out_channels = out_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__() # TODO super(UnetUp, self).__init__()\n",
    "        \n",
    "        # Create a list of layers for the upsampling block\n",
    "        # The block consists of a ConvTranspose2d layer for upsampling, followed by two ResidualConvBlock layers\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, 2, 2),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        # Concatenate the input tensor x with the skip connection tensor along the channel dimension\n",
    "        # Here, we concatenate the feature map x, which has undergone downsampling operations, \n",
    "        # with the corresponding feature map skip from the encoder along the channel dimension (dim=1). \n",
    "        # By doing so, we can directly transfer the feature information extracted at different levels in the encoder to the decoder. \n",
    "        # This helps the decoder leverage more comprehensive features from the encoder when restoring image details, \n",
    "        # reducing information loss and improving the accuracy of segmentation.\n",
    "        x = torch.cat((x, skip), 1)\n",
    "        \n",
    "        # Pass the concatenated tensor through the sequential model and return the output\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetDown(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetDown, self).__init__()\n",
    "        \n",
    "        # Create a list of layers for the downsampling block\n",
    "        # Each block consists of two ResidualConvBlock layers, followed by a MaxPool2d layer for downsampling\n",
    "        layers = [ResidualConvBlock(in_channels, out_channels), ResidualConvBlock(out_channels, out_channels), nn.MaxPool2d(2)]\n",
    "        \n",
    "        # Use the layers to create a sequential model\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the sequential model and return the output\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enhance the performance and flexibility of the diffusion model, UNet can take in more informations in the form of embeddings, such as:\n",
    "- Time embeddings: related to the time steps and noise level, as discussed earlier, the noise level controlling parameter $\\beta$ increases with the timesteps.\n",
    "- Context embeddings: provide additional context for the generation process, such as text descriptions or other conditional inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedFC(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim):\n",
    "        \n",
    "        super().__init__() # TODO super(EmbedFC, self).__init__()\n",
    "        '''\n",
    "        This class defines a generic one layer feed-forward neural network for embedding input data of\n",
    "        dimensionality input_dim to an embedding space of dimensionality emb_dim.\n",
    "        '''\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # define the layers for the network\n",
    "        layers = [\n",
    "            nn.Linear(input_dim, emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "        ]\n",
    "        \n",
    "        # create a PyTorch sequential model consisting of the defined layers\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten the input tensor\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        # apply the model layers to the flattened tensor\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextUnet(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat=256, n_cfeat=10, height=28):  # cfeat - context features\n",
    "        super(ContextUnet, self).__init__()\n",
    "\n",
    "        # number of input channels, number of intermediate feature maps and number of classes\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_cfeat = n_cfeat\n",
    "        self.h = height  #assume h == w. must be divisible by 4, so 28,24,20,16...\n",
    "\n",
    "        # Initialize the initial convolutional layer\n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
    "\n",
    "        # Initialize the down-sampling path of the U-Net with two levels\n",
    "        self.down1 = UnetDown(n_feat, n_feat)        # down1 #[10, 256, 8, 8]\n",
    "        self.down2 = UnetDown(n_feat, 2 * n_feat)    # down2 #[10, 256, 4,  4]\n",
    "        \n",
    "         # original: self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\n",
    "        self.to_vec = nn.Sequential(\n",
    "            nn.AvgPool2d((4)), \n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "        # Embed the timestep and context labels with a one-layer fully connected neural network\n",
    "        self.timeembed1 = EmbedFC(1, 2*n_feat)\n",
    "        self.timeembed2 = EmbedFC(1, 1*n_feat)\n",
    "        self.contextembed1 = EmbedFC(n_cfeat, 2*n_feat)\n",
    "        self.contextembed2 = EmbedFC(n_cfeat, 1*n_feat)\n",
    "\n",
    "        # Initialize the up-sampling path of the U-Net with three levels\n",
    "        self.up0 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, self.h//4, self.h//4), # up-sample  \n",
    "            nn.GroupNorm(8, 2 * n_feat), # normalize                       \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
    "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
    "\n",
    "        # Initialize the final convolutional layers to map to the same number of channels as the input image\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1), # reduce number of feature maps   #in_channels, out_channels, kernel_size, stride=1, padding=0\n",
    "            nn.GroupNorm(8, n_feat), # normalize\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1), # map to same number of channels as input\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, c=None):\n",
    "        \"\"\"\n",
    "        x : (batch, n_feat, h, w) : input image\n",
    "        t : (batch, n_cfeat)      : time step\n",
    "        c : (batch, n_classes)    : context label\n",
    "        \"\"\"\n",
    "        # x is the input image, c is the context label, t is the timestep, context_mask says which samples to block the context on\n",
    "\n",
    "        # pass the input image through the initial convolutional layer\n",
    "        x = self.init_conv(x)\n",
    "        # pass the result through the down-sampling path\n",
    "        down1 = self.down1(x)       #[10, 256, 8, 8]\n",
    "        down2 = self.down2(down1)   #[10, 256, 4, 4]\n",
    "        \n",
    "        # convert the feature maps to a vector and apply an activation\n",
    "        hiddenvec = self.to_vec(down2)\n",
    "        \n",
    "        # mask out context if context_mask == 1\n",
    "        if c is None:\n",
    "            c = torch.zeros(x.shape[0], self.n_cfeat).to(x)\n",
    "            \n",
    "        # embed context and timestep\n",
    "        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)     # (batch, 2*n_feat, 1,1)\n",
    "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
    "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n",
    "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
    "        #print(f\"uunet forward: cemb1 {cemb1.shape}. temb1 {temb1.shape}, cemb2 {cemb2.shape}. temb2 {temb2.shape}\")\n",
    "\n",
    "\n",
    "        up1 = self.up0(hiddenvec)\n",
    "        up2 = self.up1(cemb1*up1 + temb1, down2)  # add and multiply embeddings\n",
    "        up3 = self.up2(cemb2*up2 + temb2, down1)\n",
    "        out = self.out(torch.cat((up3, x), 1))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "# diffusion hyperparameters\n",
    "timesteps = 500\n",
    "beta1 = 1e-4\n",
    "beta2 = 0.02\n",
    "\n",
    "# network hyperparameters\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else torch.device('cpu'))\n",
    "n_feat = 64 # 64 hidden dimension feature\n",
    "n_cfeat = 5 # context vector is of size 5\n",
    "height = 16 # 16x16 image\n",
    "save_dir = './weights/'\n",
    "\n",
    "# training hyperparameters\n",
    "batch_size = 100\n",
    "n_epoch = 32\n",
    "lrate=1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct DDPM noise schedule\n",
    "b_t = (beta2 - beta1) * torch.linspace(0, 1, timesteps + 1, device=device) + beta1\n",
    "a_t = 1 - b_t\n",
    "ab_t = torch.cumsum(a_t.log(), dim=0).exp()    \n",
    "ab_t[0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct model\n",
    "nn_model = ContextUnet(in_channels=3, n_feat=n_feat, n_cfeat=n_cfeat, height=height).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. load the pretrained model without training ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in model weights and set to eval mode\n",
    "nn_model.load_state_dict(torch.load(f\"{save_dir}/model_trained.pth\", map_location=device))\n",
    "nn_model.eval()\n",
    "print(\"Loaded in Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Train it anyway ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    # from [0,255] to range [0.0,1.0]\n",
    "    transforms.Normalize((0.5,), (0.5,))  \n",
    "    # range [-1,1]\n",
    "\n",
    "])\n",
    "\n",
    "# load dataset and construct optimizer\n",
    "dataset = CustomDataset(\"./sprites_1788_16x16.npy\", \"./sprite_labels_nc_1788_16x16.npy\", transform, null_context=False)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "optim = torch.optim.Adam(nn_model.parameters(), lr=lrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function: perturbs an image to a specified noise level\n",
    "def perturb_input(x, t, noise):\n",
    "    return ab_t.sqrt()[t, None, None, None] * x + (1 - ab_t[t, None, None, None]) * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training without context code\n",
    "\n",
    "# set into train mode\n",
    "nn_model.train()\n",
    "\n",
    "for ep in range(n_epoch):\n",
    "    print(f'epoch {ep}')\n",
    "    \n",
    "    # linearly decay learning rate\n",
    "    optim.param_groups[0]['lr'] = lrate*(1-ep/n_epoch)\n",
    "    \n",
    "    pbar = tqdm(dataloader, mininterval=2 )\n",
    "    for x, _ in pbar:   # x: images\n",
    "        optim.zero_grad()\n",
    "        x = x.to(device)\n",
    "        \n",
    "        # perturb data\n",
    "        noise = torch.randn_like(x)\n",
    "        t = torch.randint(1, timesteps + 1, (x.shape[0],)).to(device) # timestap\n",
    "        x_pert = perturb_input(x, t, noise)\n",
    "        \n",
    "        # use network to recover noise\n",
    "        pred_noise = nn_model(x_pert, t / timesteps) \n",
    "        \n",
    "        # loss is mean squared error between the predicted and true noise\n",
    "        loss = F.mse_loss(pred_noise, noise)\n",
    "        loss.backward()\n",
    "        \n",
    "        optim.step()\n",
    "\n",
    "    # save model periodically\n",
    "    if ep%4==0 or ep == int(n_epoch-1):\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.mkdir(save_dir)\n",
    "        torch.save(nn_model.state_dict(), save_dir + f\"model_{ep}.pth\")\n",
    "        print('saved model at ' + save_dir + f\"model_{ep}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function; removes the predicted noise (but adds some noise back in to avoid collapse)\n",
    "def denoise_add_noise(x, t, pred_noise, z=None):\n",
    "    if z is None:\n",
    "        z = torch.randn_like(x)\n",
    "    noise = b_t.sqrt()[t] * z\n",
    "    mean = (x - pred_noise * ((1 - a_t[t]) / (1 - ab_t[t]).sqrt())) / a_t[t].sqrt()\n",
    "    return mean + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample using standard algorithm\n",
    "@torch.no_grad()\n",
    "def sample_ddpm(n_sample, save_rate=20):\n",
    "    # x_T ~ N(0, 1), sample initial noise\n",
    "    samples = torch.randn(n_sample, 3, height, height).to(device)  \n",
    "\n",
    "    # array to keep track of generated steps for plotting\n",
    "    intermediate = [] \n",
    "    for i in range(timesteps, 0, -1):\n",
    "        print(f'sampling timestep {i:3d}', end='\\r')\n",
    "\n",
    "        # reshape time tensor\n",
    "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
    "\n",
    "        # sample some random noise to inject back in. For i = 1, don't add back in noise\n",
    "        z = torch.randn_like(samples) if i > 1 else 0\n",
    "\n",
    "        eps = nn_model(samples, t)    # predict noise e_(x_t,t)\n",
    "        samples = denoise_add_noise(samples, i, eps, z)\n",
    "        if i % save_rate ==0 or i==timesteps or i<8:\n",
    "            intermediate.append(samples.detach().cpu().numpy())\n",
    "\n",
    "    intermediate = np.stack(intermediate)\n",
    "    return samples, intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize samples\n",
    "plt.clf()\n",
    "samples, intermediate_ddpm = sample_ddpm(32)\n",
    "animation_ddpm = plot_sample(intermediate_ddpm,32,4,save_dir, \"ani_run\", None, save=False)\n",
    "HTML(animation_ddpm.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acknowledgements & References\n",
    "The images in this note refer to the slide of Prof. Hongyi Li\n",
    "This code is modified from https://learn.deeplearning.ai/courses/diffusion-models/lesson/jnak3/controlling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
